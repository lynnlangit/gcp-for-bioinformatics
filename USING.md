
## Usage Patterns in GCP for Bioinformatics

This page describes some common patterns and use cases for bioinformatic research on the â˜ï¸Google Cloud Platform.
If you're new to working with public cloud services, including those available from â˜ï¸GCP, you can start by considering the three main categories of services available for you to use in cloud computing:
- ğŸ”·**Storage** - for files or data - often referred to as 'buckets'
- ğŸ”¶**Compute** - for calculation at scale - often called 'VMs or virtual machines'.  Docker container images are part of this category as well.
- ğŸ”´**Other Services** - other services, such as security configuration (permissions and encyrption) 



### 1. Using public Genomic Reference Datasets

â˜ï¸GCP hosts a number of genomic reference datasets.  You can view the files by using a â˜ï¸GCP client (Web UI or script) to browse to publically available â˜ï¸GCP Cloud Storage buckets.  Alternatively, you can write a SQL query to view any subset of the public genomic datasets available in â˜ï¸GCP BigQuery storage.  For more information see ğŸ“—[this page](https://github.com/lynnlangit/gcp-for-bioinformatics/blob/master/1_Files_%26_Data/2_Use_public_genomic_datasets.md) of this repository. A key advantage to using these datasets in your cloud-based research, is that you do not have to upload these large (or huge) files to GCP.  You can simply refer to the location, i.e. `gs://{public-bucket-name}/{folder-name}/file....` in your research job configuration.

###  2. Quickly trying out bioinformatics tools

Rather than installing bioinformatics scripts, tools or libraries and their dependencies on your local laptop, you can use â˜ï¸GCP to quickly start a 'development machine'. Do this by using services such as ğŸ”¶Google Compute Engine.  

Additionally, many â˜ï¸GCP services, such as ğŸ”¶Compute Engine or Kubernetes Engine support running docker container images, such as those on biocontainers, which include commonly used bioinformatics tools such as blastn, bamstats, GATK, Hail and other.  This means you can 'try out' docker containers on â˜ï¸GCP, rather than having to install and configure docker tools locally.

For more information see ğŸ“—[this section](https://github.com/lynnlangit/gcp-for-bioinformatics/tree/master/2_Virtual_Machines_%26_Docker_Containers) of this repository.


### 3.  Running Jupyter Notebooks 

There are a number of options for running Jupyter notebooks using â˜ï¸GCP services.  These include starting specially pre-configured, â˜ï¸GCP Notebook instances, as well as using bioinforamtics environments, such as The Broad Institute's Terra.bio Notebooks (which runs on top of GCP services.)  These instances already include the Jupyter libraries, so you don't have to install anything to be able to run Jupyter notebooks on them.

Also, notebooks which run computationally intensive analysis, such as machine learning with deep neural network libraries (i.e. TensorFlow) can be run in GCP with more powerful processors (GPUs or TPUs, in addtion to CPUs) without you having to install hardware drivers on a machine.

For more information see ğŸ“—[this page for Notebook instances](https://github.com/lynnlangit/gcp-for-bioinformatics/blob/master/2_Virtual_Machines_%26_Docker_Containers/2_Use_Jupyter_Notebook_VM.md), and ğŸ“—[this page for Terra.bio notebooks](https://github.com/lynnlangit/gcp-for-bioinformatics/blob/master/2_Virtual_Machines_%26_Docker_Containers/3_Use_Terra.bio_Notebooks.md) in this repository.


### 4.  Running Bioinformatics Analysis Jobs at scale

A key reason to move from local to cloud-based computing is the ability to scale out large-sized analysis jobs. Running on â˜ï¸GCP, can address the 'my job won't run on my laptop (or on-premise cluster)' problem. However, efficiently scaling compute can be quite complex.   Also there are many different cloud services and methods available to do this on GCP (ğŸ”¶Compute Engine VM auto-scalers, Kubernetes clusters for docker container images....).  

These scaling patterns include everything from low-level virtual machine orchestration (where the user needs to provide the majority of the configuration information) all the way to much higher-level tools that have been specifically designed to reduce the amount of manual configuration you need to do to run your jobs, such as Terra.bio from The Broad Institute and others.

For more information see ğŸ“—[this section](https://github.com/lynnlangit/gcp-for-bioinformatics/tree/master/2_Virtual_Machines_%26_Docker_Containers) of this repository.
